{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85ac91e1-f9f4-4f29-bd9f-d2d2a00c46be",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.409724Z",
     "iopub.status.busy": "2024-04-09T05:53:17.408969Z",
     "iopub.status.idle": "2024-04-09T05:53:17.415845Z",
     "shell.execute_reply": "2024-04-09T05:53:17.414328Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.409674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "from ipex_llm.transformers import AutoModel, AutoModelForCausalLM\n",
    "from modelscope import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7ed0e82-9786-4c2e-b7f8-d552563e76ee",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.418291Z",
     "iopub.status.busy": "2024-04-09T05:53:17.417935Z",
     "iopub.status.idle": "2024-04-09T05:53:17.422098Z",
     "shell.execute_reply": "2024-04-09T05:53:17.421400Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.418254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHATGLM_V3_PROMPT_FORMAT = \"<|user|>\\n{prompt}\\n<|assistant|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b2418c3-4adf-4ce8-ae73-1c18b13a50aa",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.423245Z",
     "iopub.status.busy": "2024-04-09T05:53:17.422980Z",
     "iopub.status.idle": "2024-04-09T05:53:17.427721Z",
     "shell.execute_reply": "2024-04-09T05:53:17.427054Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.423218Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_path):\n",
    "    # Load model in 4 bit,\n",
    "    # which convert the relevant layers in the model into INT4 format\n",
    "    model = AutoModel.from_pretrained(model_path,\n",
    "                                      load_in_4bit=True,\n",
    "                                      trust_remote_code=True,\n",
    "                                      model_hub='modelscope')\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path,\n",
    "                                              trust_remote_code=True)\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f473dfb8-f3d0-47d9-a8c3-d0ea0977a473",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.428839Z",
     "iopub.status.busy": "2024-04-09T05:53:17.428587Z",
     "iopub.status.idle": "2024-04-09T05:53:17.435147Z",
     "shell.execute_reply": "2024-04-09T05:53:17.434527Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.428813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_tokens(model, tokenizer, prompt, n_predict):\n",
    "    # Generate predicted tokens\n",
    "    prompt = CHATGLM_V3_PROMPT_FORMAT.format(prompt=prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        st = time.time()\n",
    "        output = model.generate(input_ids, max_new_tokens=n_predict)\n",
    "        end = time.time()\n",
    "\n",
    "    output_str = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f'Inference time: {end-st} s')\n",
    "    print('-'*20, 'Prompt', '-'*20)\n",
    "    print(prompt)\n",
    "    print('-'*20, 'Output', '-'*20)\n",
    "    print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc950638-e8be-4fb0-928b-e8650ba241af",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.437251Z",
     "iopub.status.busy": "2024-04-09T05:53:17.436993Z",
     "iopub.status.idle": "2024-04-09T05:53:17.440448Z",
     "shell.execute_reply": "2024-04-09T05:53:17.439799Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.437225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = 'ZhipuAI/chatglm3-6b'\n",
    "prompt = 'AI是什么？'\n",
    "n_predict = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0595430f-189a-417f-a5c8-79ab85dd6040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:17.441664Z",
     "iopub.status.busy": "2024-04-09T05:53:17.441410Z",
     "iopub.status.idle": "2024-04-09T05:53:51.379374Z",
     "shell.execute_reply": "2024-04-09T05:53:51.378618Z",
     "shell.execute_reply.started": "2024-04-09T05:53:17.441638Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 13:53:17,773 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n",
      "2024-04-09 13:53:18,412 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.27s/it]\n",
      "2024-04-09 13:53:27,812 - INFO - Converting the current model to sym_int4 format......\n",
      "2024-04-09 13:53:50,917 - modelscope - WARNING - Model revision not specified, use revision: v1.0.2\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fbda371-f426-475c-b0fd-69d88b6d8bc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-09T05:53:51.380722Z",
     "iopub.status.busy": "2024-04-09T05:53:51.380398Z",
     "iopub.status.idle": "2024-04-09T06:01:13.613022Z",
     "shell.execute_reply": "2024-04-09T06:01:13.608115Z",
     "shell.execute_reply.started": "2024-04-09T05:53:51.380693Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 442.2059109210968 s\n",
      "-------------------- Prompt --------------------\n",
      "<|user|>\n",
      "AI是什么？\n",
      "<|assistant|>\n",
      "-------------------- Output --------------------\n",
      "[gMASK]sop <|user|>\n",
      "AI是什么？\n",
      "<|assistant|> AI是人工智能（Artificial Intelligence）的缩写，指的是通过计算机程序和算法实现智能的一种技术。AI可以帮助人类完成各种任务，例如语音\n"
     ]
    }
   ],
   "source": [
    "generate_tokens(model, tokenizer, prompt, n_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipex-llm-test",
   "language": "python",
   "name": "ipex-llm-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
